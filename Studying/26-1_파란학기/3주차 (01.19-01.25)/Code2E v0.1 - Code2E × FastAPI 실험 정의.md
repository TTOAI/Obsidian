
## 실험의 목적

> “AI가 백엔드 개발 사이클을 자율적으로 수행할 때  
> 어디까지가 신뢰 가능하고,  
> 어디서부터 인간 개입이 필수적으로 요구되는지를  
> End-to-End 실행과 기록을 통해 밝히는 것”이다.

---

## 실험의 최상위 목표

> 최소한의 백엔드 프레임워크 환경에서  
> AI가 요구사항 기반 개발 사이클을  
> 자율적으로 반복 수행하며  
> 그 성공·실패·수렴 과정을  
> 코드, 실행 로그, 테스트 결과, 커밋 이력으로  
> 명확히 기록할 수 있는지를 검증하는 것이다.

---

## 실패의 정의

- F1. 원인 없는 무한 반복
	- 동일한 오류를 해결하지 못한 채 이유 설명 없이 반복
    - → **Code2E 구조 문제**
- F2. 실패는 발생했지만 해석되지 않음

- 로그는 있음
    
- 테스트 결과는 있음
    
- 그러나 “왜 이 지점에서 멈췄는지” 설명 불가
    

→ **평가 레이어 부족**

---

### ❌ F3. 결과물은 코드뿐이고 과정이 없음

- 최종 코드만 존재
    
- 실패/수정/결정 기록 없음
    

→ **Code2E 정체성 붕괴**

---

## 4️⃣ Human-in-the-loop 개입의 정의

### ✔ 허용되는 인간 개입

- 최초 요구사항/명세 입력
    
- 실험 종료 후 결과 해석
    
- “이 커밋부터 내가 작업” 결정
    

### ❌ 허용되지 않는 인간 개입

- 코드 직접 수정
    
- 테스트 직접 추가
    
- 실행 실패 수동 해결
    

> 이 제한이 있어야  
> **Human-out-of-the-loop 실험이 의미를 갖는다.**

---

## 5️⃣ FastAPI 실험의 Scope 명시 (중요)

### ✔ 포함

- 단일 FastAPI 앱
    
- 단일 또는 소수 API
    
- 로컬 실행
    
- pytest 기반 테스트
    

### ❌ 제외 (명시적으로)

- 인증/인가
    
- 외부 API
    
- 분산 시스템
    
- 성능 최적화
    

> ❌ 안 하는 것을 명시하는 건  
> **프로젝트 완성도를 낮추는 게 아니라 높이는 행위다.**

---

## 6️⃣ 최종 정의 요약 (README/제안서용)

> **Code2E × FastAPI 실험은  
> 최소한의 백엔드 환경에서  
> AI가 개발 사이클을 자율적으로 반복 수행할 때  
> 자동화가 성공하는 지점과  
> 반드시 인간 개입이 필요한 경계를  
> 실행·테스트·수정·커밋 로그로  
> 설명할 수 있는지를 검증하는 실험이다.**