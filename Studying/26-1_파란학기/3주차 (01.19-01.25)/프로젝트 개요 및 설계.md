## 주제 의식

- AI는 인간의 개입 없이 개발 사이클 전체를 안정적으로 닫을 수 있는가?
- 인간이 개입하지 않았을 때, AI 개발 시스템은 어떤 지점에서, 어떤 이유로 신뢰를 잃는가?
- 자동화 수준을 어떻게 조절해야 생산성이 최대가 되는가?
- 적절한 Human-in-the-loop의 기준은 무엇인가?
---

## 기존 코딩 에이전트의 한계

- 테스트/검증은 보조적임
	- `테스트/검증이 보조적으로 제공되는 에이전트가 있는가? 혹은 시스템 내부에서 테스트/검증이 이루어지는가?`
- 코드 생성과 평가가 동일한 맥락(context)에 노출되어 있음
- 실패의 원인, 수렴 여부, 신뢰성에 대한 체계적인 분석을 제공하지 않음
	- `수렴 여부는 어떻게 확인할 것인가?`
		- 수렴 정의: 동일 요구사항 집합에 대해 수정 → 실행 → 테스트 루프가 반복되었을 때 실패 유형이 감소하거나, 동일 실패가 반복되어 '더 이상 개선되지 않음'이 명확해지는 상태
	- `체계적인 분석 제공 -> 실패 코드, 어떻게 실패했는지, 왜 실패했는지, 어떻게 고치는지, 수정 이후에 고쳐졌는지, 다른 테스트와는 독립적으로 이루어졌는지, 성공했던 테스트가 다시 실패하지 않는지, 최종적으로 모든 요구사항을 만족하는지 등`
- 따라서 기존 AI 에이전트는 Human-in-the-loop가 필수적임
---

## 문제 의식

- 코드를 얼마나 잘 짜는가 (X)
- AI 개발 자동화가 언제, 왜, 어떻게 실패하는가를 설명할 수 있는가 (O)
---

## 프로젝트 목표

- 개발 사이클 자동화의 한계와 가능성 검증
- 단순 성공/실패가 아니라 수렴 여부와 반복 특성을 관찰
	- `반복 특성이란 무엇인가? -> 반복 특성 예시`
		- 동일 실패 반복 횟수
		- 수정 diff 크기 변화
		- 테스트 통과율 변화
		- 실패 유형의 다양성 변화
- 신뢰 가능한 자동화의 조건 정의
	- 테스트 기반 검증, 실패 통제, 중단 조건(stopping criteria)을 통해 무한 루프·환각·편향을 제어할 수 있는 구조를 설계
		- `실패 통제와 중단 조건은 어떻게 설계할 것인가?`
			- Decision Layer에서 정의
			- stopping criteria 예시 (설명 가능하고 재현 가능한 기준선(baseline)으로 정의)
				- 동일 실패 N회 반복
				- 수정 후 테스트 통과율 감소
				- 수정 범위 확대 대비 성과 없음
				- 실행 성공했지만 요구사항 미충족 지속
		- `환각은 어떻게 통제할 것인가? -> 테스트 기준을 정립하거나, 코드 유형별 테스트 기법 표준을 도입하여 RAG 기반 결과 도출?`
			- 환각: 실제 실행·테스트 결과에 의해 뒷받침되지 않는 상태를 AI가 성공 또는 해결로 오인하는 현상
			- 제어 방법
				- 핵심 원칙
					- 실행·테스트로 증명되지 않은 상태는 어떤 경우에도 ‘성공’으로 간주하지 않는다.
			- 구조적 제어 방법
				- Execution Layer를 단일 Source of Truth로 설정
				    - LLM의 추론, 설명, 자신감은 판단 근거가 아님
				    - 실제 실행 결과(서버 기동, 테스트 통과/실패)만 사용
				- 테스트 기반 검증 강제
				    - 테스트 통과 없이 “고쳐졌다” 판단 불가
				    - 일부 테스트 통과 ≠ 전체 해결
				- Decision Layer에서 성공 선언 제한
				    - 실행 로그 + 테스트 결과 + 요구사항 매핑이 모두 있어야 다음 단계 진행
				    - 증거 없는 “해결됨” 선언 차단
		- `편향은 어떻게 측정하는가? 어디에 편향되었다는 것의 기준이 무엇인가? -> 블랙박스 테스트, 블라인드 구조를 통한 자기 검증 편향 제어 및 에이전트 간 정보 비대칭 유지`
			- 편향: 코드 생성·검증·수정이 동일한 관점에 의해 이루어지면서 특정 해결 경로가 자기 강화되는 구조적 문제
			- 제어 방법
				- Blind 구조 (정보 비대칭 유지)
					- **Code Agent**
					    - 테스트 코드·세부 실패 원인 접근 불가
					    - “무엇이 실패했는지”의 요약만 전달받음
					- **Test Agent**
					    - 코드 내부 구조 접근 불가
					    - 구현 의도를 추측하지 않고 블랙박스 기준으로 검증
				- 역할 분리 기반 수정 루프
					- Code Agent는 “구현 관점”
					- Test Agent는 “검증 관점”
					- 둘 사이의 상호작용은 **Master가 중재**
				- 반복 패턴 감지 및 중단
					- 동일한 수정 전략 반복
					- 실패 유형 변화 없음
					- 수정 범위 확대 대비 성과 없음
					- → Decision Layer에서 자동 중단
	- "돌아가는 코드"가 아닌 검증된 결과를 기준으로 자동화 판단
		- `검증된 결과란 무엇인가? 결과를 어떻게 검증할 것인가?`
			- 검증된 결과: 실제 실행 환경에서 테스트·로그·요구사항 매핑을 통해 자동화 판단의 근거로 사용 가능한 결과
- 비교·평가 가능한 실험 시스템 구축
	- 특정 도구/모델에 종속되지 않고 모델·런타임·오케스트레이션을 교체 가능한 구조로 설계
	- 구조 차이에 따른 성능·신뢰성 차이를 실험적으로 비교
		- `성능을 어떤 기준으로 측정할 것인가?`
			- 성능 1차 목표
				- 반복 횟수
				- 수렴까지 걸린 step 수
				- 자동 수정 성공률
		- `신뢰성을 어떤 기준으로 측정할 것인가?`
			- 신뢰성 1차 목표
				- 테스트 재실행 시 결과 안정성
				- 회귀 실패 여부
				- 실패 유형의 설명 가능성
---

## 프로젝트 개요

- Code2E: AI가 개발 사이클 전 과정을 자율적으로 수행할 수 있는지를 검증하기 위한 멀티 에이전트 기반 개발 자동화 실험 프레임워크
- 특징
	- 코드 생성과 검증을 의도적으로 분리한 에이전트 구조
	- 테스트·정적분석·회귀 검증을 자동 수행하는 평가 레이어
	- 실패 유형, 반복 횟수, 수렴 여부를 실험 데이터로 기록
	- LLM, 런타임, 오케스트레이션을 교체 가능한 모듈 구조로 설계
---

## 아키텍처 설계 원칙

- 통합보다 분리
	- 코드 생성과 테스트/검증을 분리하여 자기 검증 편향(self-verification bias) 최소화
	- 에이전트 간 정보 비대칭을 의도적으로 유지 (Blind 구조)
- 도구 중심이 아니라 구조 중심
	- "많은 프레임워크 사용"이 목표가 아님
	- 각 레이어별로 최소 구성 + 교체 가능성을 우선
- 결과보다 과정 기록
	- 성공 여부보다 실패의 형태와 반복 경로를 핵심 산출물로 취급
	- 로그·메타데이터·실험 조건을 구조화해 저장
---

## 아키텍처 구조

- 전체 레이어 개요
```
[Human Input]
     ↓
[Orchestrator / Master Agent]
     ↓
 ┌───────────────┐
 │  Code Agent   │  ← 구현 전담 (Blind)
 └───────────────┘
        ↓
 ┌─────────────────┐
 │ Execution Layer │  ← 실제 실행 (서버/테스트)
 └─────────────────┘
        ↓
 ┌───────────────┐
 │  Test Agent   │  ← 검증·해석 전담 (Blind)
 └───────────────┘
        ↓
[Decision Layer]
     ↓
[Final Agent / Artifact Layer]
```
---

## 핵심 컴포넌트 설명

- Human Input Layer
	- 프로젝트 개요
	- 요구사항 (FR / NFR)
	- API 명세
	- 성공/실패 기준
	- 실험 목적 정의

- Master Agent
	- 전체 개발 루프 제어
	- Agent 호출 순서 관리
	- 반복 횟수 제한
	- Decision Layer 결과에 따른 제어
	- 실험 단위(run) 관리
	- 모든 메타데이터 수집

- Code Agent
	- 요구사항 기반 코드 생성
	- 기존 코드 수정
	- 테스트 결과의 "요약"만 전달받음
	- 테스트 코드/세부 실패 원인 직접 접근 불가

- Execution Layer
	- Master Agent에 의해 호출
	- FastAPI 서버 실행
	- pytest 테스트 실행
	- 예외/에러 캡처
	- 로그 수집
	- 실행 결과만 반환

- Test Agent
	- 테스트 코드 생성
	- 테스트 결과(Execution Layer의 결과) 해석
	- 실패 유형 분류
	- 다음 수정 방향 제안
	- 코드 내부 구조 직접 접근 불가

- Decision Layer
	- 판단 로직(Policy) 정립
	- 규칙·지표 기반 결론 도출
	- 계속 자동 수정할지, 중단할지, Human escalation이 필요한지 결정
	- 판단 기준 예:
		- 동일 실패 반복
		- 테스트-실행 불일치
		- 수정 비용 증가
		- 수렴 가능성 저하

- Final Agent
	- 최종 코드 상태 정리
	- 중간 코드 스냅샷 정리
	- Git 커밋 로그 수집·요약
		- 단순한 코드 버전 관리를 넘어 자동화 가능 지점을 시간적으로 고정하는 실험 증거로 사용
	- 실행 로그·테스트 결과 정리
	- 요구사항 충족 매핑
	- 최종 보고서 생성
---

## 실험

- 좋은 실험의 조건
	- 실행하지 않으면 모르는 오류가 반드시 존재
	    - 정적 분석/추론만으로는 판단 불가
	- 전체가 아닌 부분적인 성공/실패 상태가 자연스럽게 발생
	- 테스트가 결과를 정의하는 권위를 가짐
	- 수정 → 재실행 → 재검증 루프가 의미 있음
	- 잘못된 자동화가 오히려 생산성을 떨어뜨릴 수 있음
	    - Human-in-the-loop 경계가 명확히 드러남

- 실험 유형
	- 백엔드 서비스 개발 (FastAPI / Spring Boot)
		- 실행하지 않으면 모르는 오류가 많음
		- 테스트가 곧 진실
		- 부분 성공 상태 풍부
		- 잘못된 자동 수정이 생산성을 확실히 떨어뜨림
		- Human-in-the-loop 경계가 명확
	- CLI 기반 시스템 / 도구
		- 입력 → 출력 명확
		- 실행/에러 로그 풍부
		- 테스트/회귀 적합
	- 데이터 파이프라인 / ETL
		- 단계별 실패 명확
		- 부분 성공 구조 자연스러움
		- 자동화 vs 신뢰성 문제 선명

- 단계별 확장
	0. 구조 검증
		- FastAPI 단일 API
		- 실패/중단/설명 증명
		- 목표: 자동화가 멈추는 지점을 설명할 수 있는가?
	1. 실행 도메인 확장
		- FastAPI 복수 API
		- 입력 검증 / 에러 처리 강화
		- Decision Layer 고도화
		- 목표: 자동화 가능 구간이 어디까지 확장되는가?
	2. 개발 유형 확장
		- CLI 도구 개발
		- 데이터 파이프라인 추가
		- 동일 아키텍처로 실험
		- 목표: 이 구조는 다른 개발 유형에서도 작동하는가?
	3. 범용 시스템화
		- 개발 유형을 "플러그인"으로 추상화
		- 공통 개발 사이클 모델 제공
		- Decision Layer 정책 교체 가능
		- 목표: Decision Layer 정책 교체 가능
