### 주제 의식

- AI는 인간의 개입 없이 개발 사이클 전체를 안정적으로 닫을 수 있는가?
- 인간이 개입하지 않았을 때, AI 개발 시스템은 어떤 지점에서, 어떤 이유로 신뢰를 잃는가?
- 자동화 수준을 어떻게 조절해야 생산성이 최대가 되는가?
- 적절한 Human-in-the-loop의 기준은 무엇인가?
---

### 기존 코딩 에이전트의 한계

- 테스트/검증은 보조적임
	- `테스트/검증이 보조적으로 제공되는 에이전트가 있는가? 혹은 시스템 내부에서 테스트/검증이 이루어지는가?`
- 코드 생성과 평가가 동일한 맥락(context)에 노출되어 있음
- 실패의 원인, 수렴 여부, 신뢰성에 대한 체계적인 분석을 제공하지 않음
	- `수렴 여부는 어떻게 확인할 것인가?`
	- `체계적인 분석 제공 -> 실패 코드, 어떻게 실패했는지, 왜 실패했는지, 어떻게 고치는지, 수정 이후에 고쳐졌는지, 다른 테스트와는 독립적으로 이루어졌는지, 성공했던 테스트가 다시 실패하지 않는지, 최종적으로 모든 요구사항을 만족하는지 등`
- 따라서 기존 AI 에이전트는 Human-in-the-loop가 필수적임
---

### 문제 의식

- 코드를 얼마나 잘 짜는가 (X)
- AI 개발 자동화가 언제, 왜, 어떻게 실패하는가를 설명할 수 있는가 (O)
---

### 프로젝트 목표

- 개발 사이클 자동화의 한계와 가능성 검증
- 단순 성공/실패가 아니라 수렴 여부와 반복 특성을 관찰
	- `수렴 여부는 어떻게 판단할 것인가?`
	- `반복 특성이란 무엇인가?`
- 신뢰 가능한 자동화의 조건 정의
	- 테스트 기반 검증, 실패 통제, 중단 조건(stopping criteria)을 통해 무한 루프·환각·편향을 제어할 수 있는 구조를 설계
		- `실패 통제와 중단 조건은 어떻게 설계할 것인가?`
		- `무한 루프 제어 -> limit?`
		- `환각은 어떻게 통제할 것인가? -> 테스트 기준을 정립하거나, 코드 유형별 테스트 기법 표준을 도입하여 RAG 기반 결과 도출?`
		- `편향은 어떻게 측정하는가? 어디에 편향되었다는 것의 기준이 무엇인가? -> 블랙박스 테스트, 블라인드 구조를 통한 자기 검증 편향 제어 및 에이전트 간 정보 비대칭 유지`
	- "돌아가는 코드"가 아닌 검증된 결과를 기준으로 자동화 판단
		- `검증된 결과란 무엇인가? 결과를 어떻게 검증할 것인가?`
- 비교·평가 가능한 실험 시스템 구축
	- 특정 도구/모델에 종속되지 않고 모델·런타임·오케스트레이션을 교체 가능한 구조로 설계
	- 구조 차이에 따른 성능·신뢰성 차이를 실험적으로 비교
		- `성능을 어떤 기준으로 측정할 것인가?`
		- `신뢰성을 어떤 기준으로 측정할 것인가?`
---

### 프로젝트 개요

- Code2E: AI가 개발 사이클 전 과정을 자율적으로 수행할 수 있는지를 검증하기 위한 멀티 에이전트 기반 개발 자동화 실험 프레임워크
- 특징
	- 코드 생성과 검증을 의도적으로 분리한 에이전트 구조
	- 테스트·정적분석·회귀 검증을 자동 수행하는 평가 레이어
	- 실패 유형, 반복 횟수, 수렴 여부를 실험 데이터로 기록
	- LLM, 런타임, 오케스트레이션을 교체 가능한 모듈 구조로 설계
---

### 아키텍처 설계 원칙

- 통합보다 분리
	- 코드 생성과 테스트/검증을 분리하여 자기 검증 편향(self-verification bias) 최소화
	- 에이전트 간 정보 비대칭을 의도적으로 유지 (Blind 구조)
- 도구 중심이 아니라 구조 중심
	- "많은 프레임워크 사용"이 목표가 아님
	- 각 레이어별로 최소 구성 + 교체 가능성을 우선
- 결과보다 과정 기록
	- 성공 여부보다 실패의 형태와 반복 경로를 핵심 산출물로 취급
	- 로그·메타데이터·실험 조건을 구조화해 저장
---

### 아키텍처 구조

- 전체 레이어 개요
```
[Human Input]
     ↓
[Orchestrator / Master Agent]
     ↓
 ┌───────────────┐
 │   Code Agent  │  ← 구현 전담 (Blind)
 └───────────────┘
        ↓
 ┌────────────────┐
 │ Execution Layer │  ← 실제 실행 (서버/테스트)
 └────────────────┘
        ↓
 ┌───────────────┐
 │  Test Agent   │  ← 검증·해석 전담 (Blind)
 └───────────────┘
        ↓
[Decision Layer]
     ↓
[Final Agent / Artifact Layer]
```
---

### 핵심 컴포넌트 설명

- Human Input Layer
	- 프로젝트 개요
	- 요구사항 (FR / NFR)
	- API 명세
	- 성공/실패 기준
	- 실험 목적 정의

- Master Agent
	- 전체 개발 루프 제어
	- Agent 호출 순서 관리
	- 반복 횟수 제한
	- stopping condition 판단
	- 실험 단위(run) 관리
	- 모든 메타데이터 수집

- Code Agent
	- 요구사항 기반 코드 생성
	- 기존 코드 수정
	- 테스트 결과의 “요약”만 전달받음
	- 테스트 코드/세부 실패 원인 직접 접근 불가

- Execution Layer
	- FastAPI 서버 실행
	- pytest 테스트 실행
	- 예외/에러 캡처
	- 로그 수집
	- 실행 결과만 반환 (사실)

- Test Agent
	- 테스트 코드 생성
	- 테스트 결과 해석
	- 실패 유형 분류
	- 다음 수정 방향 제안
	- 코드 내부 구조 직접 접근 불가

- Decision Layer
	- 계속 자동 수정할지
	- 중단할지
	- Human escalation이 필요한지 결정
	- 판단 기준 예:
		- 동일 실패 반복
		- 테스트-실행 불일치
		- 수정 비용 증가
		- 수렴 가능성 저하

- Final Agent
	- 최종 코드
	- 중간 코드 스냅샷
	- Git 커밋 로그
	- 실행 로그
	- 테스트 결과
	- 요구사항 충족 매핑
	- 최종 보고서 (핵심 산출물)
