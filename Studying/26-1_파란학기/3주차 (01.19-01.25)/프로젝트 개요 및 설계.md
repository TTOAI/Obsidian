### 주제 의식

- AI는 인간의 개입 없이 개발 사이클 전체를 안정적으로 닫을 수 있는가?
---

### 기존 코딩 에이전트의 한계

- 테스트/검증은 보조적임
	- `테스트/검증이 보조적으로 제공되는 에이전트가 있는가? 혹은 시스템 내부에서 테스트/검증이 이루어지는가?`
- 코드 생성과 평가가 동일한 맥락(context)에 노출되어 있음
- 실패의 원인, 수렴 여부, 신뢰성에 대한 체계적인 분석을 제공하지 않음
	- `수렴 여부는 어떻게 확인할 것인가?`
	- `체계적인 분석 제공 -> 실패 코드, 어떻게 실패했는지, 왜 실패했는지, 어떻게 고치는지, 수정 이후에 고쳐졌는지, 다른 테스트와는 독립적으로 이루어졌는지, 성공했던 테스트가 다시 실패하지 않는지, 최종적으로 모든 요구사항을 만족하는지 등`
---

### 문제 의식

- 코드를 얼마나 잘 짜는가 (X)
- AI 개발 자동화가 언제, 왜, 어떻게 실패하는가를 설명할 수 있는가 (O)
---

### 프로젝트 목표

- 개발 사이클 자동화의 한계와 가능성 검증
- 단순 성공/실패가 아니라 수렴 여부와 반복 특성을 관찰
	- `수렴 여부는 어떻게 판단할 것인가?`
	- `반복 특성이란 무엇인가?`
- 신뢰 가능한 자동화의 조건 정의
	- 테스트 기반 검증, 실패 통제, 중단 조건(stopping criteria)을 통해 무한 루프·환각·편향을 제어할 수 있는 구조를 설계
		- `실패 통제와 중단 조건은 어떻게 설계할 것인가?`
		- `무한 루프 제어 -> limit?`
		- `환각은 어떻게 통제할 것인가? -> 테스트 기준을 정립하거나, 코드 유형별 테스트 기법 표준을 도입하여 RAG 기반 결과 도출?`
		- `편향은 어떻게 측정하는가? 어디에 편향되었다는 것의 기준이 무엇인가? -> 블랙박스 테스트, 블라인드 구조를 통한 자기 검증 편향 제어 및 에이전트 간 정보 비대칭 유지`
	- "돌아가는 코드"가 아닌 검증된 결과를 기준으로 자동화 판단
		- `검증된 결과란 무엇인가? 결과를 어떻게 검증할 것인가?`
- 비교·평가 가능한 실험 시스템 구축
	- 특정 도구/모델에 종속되지 않고 모델·런타임·오케스트레이션을 교체 가능한 구조로 설계
	- 구조 차이에 따른 성능·신뢰성 차이를 실험적으로 비교
		- `성능을 어떤 기준으로 측정할 것인가?`
		- `신뢰성을 어떤 기준으로 측정할 것인가?`
---

### 프로젝트 개요

- Code2E: AI가 개발 사이클 전 과정을 자율적으로 수행할 수 있는지를 검증하기 위한 멀티 에이전트 기반 개발 자동화 실험 프레임워크
- 특징
	- 코드 생성과 검증을 의도적으로 분리한 에이전트 구조
	- 테스트·정적분석·회귀 검증을 자동 수행하는 평가 레이어
	- 실패 유형, 반복 횟수, 수렴 여부를 실험 데이터로 기록
	- LLM, 런타임, 오케스트레이션을 교체 가능한 모듈 구조로 설계
---

### 아키텍처 설계 원칙

- 통합보다 분리
	- 코드 생성과 테스트/검증을 분리하여 자기 검증 편향(self-verification bias) 최소화
	- 에이전트 간 정보 비대칭을 의도적으로 유지 (Blind 구조)
- 도구 중심이 아니라 구조 중심
	- "많은 프레임워크 사용"이 목표가 아님
	- 각 레이어별로 최소 구성 + 교체 가능성을 우선
- 결과보다 과정 기록
	- 성공 여부보다 실패의 형태와 반복 경로를 핵심 산출물로 취급
	- 로그·메타데이터·실험 조건을 구조화해 저장
---

### 아키텍처 구조

- 전체 레이어 개요
```
	[User Spec]
		↓
[Master Orchestrator]
		↓
┌───────────────┐
│   Code Agent  │  ← 코드 생성만 담당
└───────────────┘
		↓
┌──────────────────────────┐
│ Evaluation & Test Layer  │
│  - Unit / Property Test  │
│  - Static Analysis       │
│  - Regression Check      │
└──────────────────────────┘
		↓
┌───────────────┐
│  Test Agent   │ ← 명세·엣지케이스·실패 분석
└───────────────┘
		↓
[Fix / Stop / Accept Decision]
```
---

### 핵심 컴포넌트 설명

- Master Orchestrator
	- 전체 개발 루프 제어
	- 반복 횟수 제한, 성공 조건, 중단 조건 관리
		- `성공 조건과 중단 조건은 어떻게 정할 것인가?`
	- 실험 단위(run) 관리 및 메타데이터 기록
		- `실험 단위는 어떻게 나눌 것인가?`
		- `메타데이터에는 무엇이 있는가?`

- Code Agent
	- 주어진 명세에 따라 코드만 생성
	- 테스트 결과나 내부 평가 정보는 직접 참조하지 않음

- Test Agent
	- 테스트 결과를 기반으로
	    - 실패 원인 분류
		    - `입력과 출력 결과밖에 모르는 상태에서 실패 원인을 어떻게 도출할 것인가?`
	    - 추가 테스트/엣지케이스 생성
	    - 수정 방향 제안
		    - `코드를 모르는 상태에서 수정 방향을 제시할 수 있는가?`
		    - `실패 원인 분석 및 수정 방향 도출은 Code Agent의 역할이 아닌가?`
	- 코드 자체보다는 행동 결과(output) 중심 평가

#### 4) Evaluation Layer (Code2E의 핵심)

- 자동 실행되는 검증 레이어
    
    - 단위 테스트 / property-based test
        
    - 정적 분석
        
    - 회귀 테스트
        
- 성공/실패를 **정량 지표**로 환원
    

#### 5) Model / Runtime Abstraction

- 로컬/원격 LLM 교체 가능
    
- 특정 모델 성능이 아닌 **구조 효과**를 관찰하기 위함