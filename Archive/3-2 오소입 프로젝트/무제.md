---

---

## 인사

안녕하십니까.  
소프트웨어학과 21학번 **이주영**입니다.

오늘은 **Vector DB인 Qdrant를 활용한 RAG 기반 문서 질의응답 시스템 구축 프로젝트**에 대해 발표드리겠습니다.  
이 프로젝트는 LLM의 한계를 보완하기 위한 RAG 구조를 **직접 end-to-end로 구현**하고,  
FastAPI와 Docker Compose를 활용해 **실제 서비스 형태로 동작하는 MVP를 만드는 것**을 목표로 진행했습니다.

---

## LLM의 한계

먼저 프로젝트의 배경부터 말씀드리겠습니다.

대형 언어 모델, 즉 LLM은 매우 뛰어난 언어 생성 능력을 가지고 있지만,  
여전히 몇 가지 명확한 한계를 가지고 있습니다.

대표적으로

- **학습 시점 이후의 최신 정보**,
    
- **특정 조직이나 도메인 내부 문서**,
    
- 그리고 **근거 없는 답변을 그럴듯하게 생성하는 할루시네이션 문제**가 있습니다.
    

즉, LLM은 “모른다”라고 말하기보다는  
**알고 있는 것처럼 답을 만들어내는 구조**를 가지고 있다는 점이 실서비스에서 큰 리스크가 됩니다.

---

## 이를 개선할 수 있는 방안으로서 RAG

이러한 한계를 해결하기 위한 대표적인 접근이 바로 **RAG**,  
**Retrieval-Augmented Generation**, 검색 증강 생성입니다.

위키백과의 정의를 간단히 요약하면,  
RAG는 LLM이 바로 답변을 생성하는 것이 아니라,  
**먼저 외부 문서 집합에서 관련 정보를 검색하고**,  
그 결과를 **프롬프트에 포함시켜 답변을 생성하도록 하는 구조**입니다 .

이 방식을 사용하면

- LLM은 **지정된 문서 범위 내에서만 답변**하게 되고
    
- 내부 문서, 최신 정보, 특정 도메인 지식을 **정확한 근거와 함께 활용**할 수 있습니다.
    

---

## 본 프로젝트 개요

이러한 RAG 구조를 직접 구현해보는 것이 이번 프로젝트의 핵심 목표입니다.

### 프로젝트 목표

- Vector DB 기반 검색 시스템을 직접 구성해보고
    
- RAG의 **전체 파이프라인**을 단계별로 구현하며
    
- FastAPI API + Docker + Qdrant로 **실제 서비스처럼 동작하는 MVP**를 완성하는 것이 목표였습니다.
    

### 데이터 및 사용 기술

데이터는

- **2025 아주대학교 요람(소프트웨어학과)**
    
- **학사과정 학사운영규칙 문서**를 사용했습니다.
    

사용 기술은

- Backend: **FastAPI, Sentence-Transformers, Qdrant, OpenAI GPT**
    
- Frontend: **Streamlit**
    
- 실행 및 배포: **Docker Compose**입니다.
    

---

## Vector DB 설명 및 Qdrant 소개

여기서 중요한 역할을 하는 것이 바로 **Vector Database**입니다.

Vector DB는  
텍스트나 이미지 같은 데이터를 **고차원 벡터(embedding)** 로 변환해 저장하고,  
의미적으로 유사한 데이터를 **빠르게 검색**할 수 있도록 설계된 데이터베이스입니다.

그중에서도 **Qdrant**는

- 고성능 벡터 검색
    
- 벡터와 함께 **payload(텍스트, 출처 등 메타데이터)** 를 함께 저장할 수 있어  
    RAG 시스템에 매우 적합한 Vector DB입니다 .
    

---

## 시스템 구조 및 전체 파이프라인

전체 시스템 흐름은 다음과 같습니다.

문서가 들어오면

1. **문서 정제 및 Chunking**
    
2. **Embedding 생성**
    
3. **Qdrant에 벡터 저장**
    
4. 질의 시 **Retrieval → Prompt 생성 → LLM 답변 생성**
    

즉,

> _문서 → 벡터화 → 검색 → 생성_  
> 이라는 전형적인 RAG 파이프라인을 end-to-end로 구현했습니다.

## 시연
### 실행환경 세팅 파일 설명

- 📄 `main.py` (backend)
	- FastAPI 애플리케이션의 **엔트리 포인트**
	- 기능별로 분리된 router(`ingest`, `search`, `query`, `debug` 등)를 하나의 앱으로 통합
	- 서버 상태 확인용 `/` 헬스체크 엔드포인트 제공
	👉 **역할 요약**: 백엔드 API들을 조립하고 실행하는 중심 파일
---
- 📄 `requirements.txt`
	- 백엔드에서 사용하는 **Python 라이브러리 의존성 목록**
	- FastAPI 서버, Qdrant 벡터 DB 연동, 임베딩 생성, LLM 호출에 필요한 패키지 포함
	👉 **역할 요약**: RAG 시스템 실행에 필요한 라이브러리 정의 파일
---
- 📄 `Dockerfile` (frontend)
	- Streamlit 기반 프론트엔드 애플리케이션을 **컨테이너 이미지로 빌드**
	- 필요한 라이브러리 설치 후 Streamlit 앱을 자동 실행
	- `BACKEND_URL` 환경변수를 통해 FastAPI 백엔드와 연결
	👉 **역할 요약**: 프론트엔드 UI를 독립된 컨테이너로 실행하기 위한 설정 파일
---
- 📄 `docker-compose.yml`
	- Qdrant(Vector DB), FastAPI 백엔드, Streamlit 프론트엔드를 **하나의 시스템으로 묶는 오케스트레이션 파일**
	- 서비스 간 실행 순서, 포트, 네트워크, 볼륨(데이터 영속성) 관리
	👉 **역할 요약**: RAG 전체 시스템을 한 번에 실행·관리하기 위한 핵심 구성 파일
---
- 📄 `.env` (참고용)
- API Key, 모델 설정 등 **민감한 환경변수 관리**
- 코드와 설정을 분리하여 보안성과 재현성 확보
	👉 **역할 요약**: 환경별 설정을 분리 관리하는 설정 파일
---
### text_cleaning, chunker 코드 설명

- `text_cleaning.py` — 텍스트 정제 흐름
	- 목적  
		→ 임베딩·청킹 전에 **형식적 노이즈만 제거**하고 의미는 건드리지 않음.
	- 처리 흐름
	1. 빈 입력 방어
	    - 입력이 없으면 빈 문자열 반환
	2. 탭 제거
	    - `\t` → 공백 `" "`  
	        → PDF, 크롤링 텍스트에서 흔한 탭 노이즈 제거
	3. 과도한 줄바꿈 정리
	    - `\n{3,}` → `\n\n`  
	        → 문단 구분은 유지하되, 불필요한 공백 제거
	4. 중복 공백 제거
	    - 연속된 공백(2칸 이상) → 1칸
	5. 라인 단위 정리
	    - 각 줄 양쪽 공백 제거 (`strip`)
	    - 줄 구조 자체는 유지
	6. 전체 trim
	    - 문서 앞뒤 불필요한 공백 제거

- chunker.py` — 청크 생성 전략

```
원문
 → normalize_text
 → 문단 분리
 → 문단 단위로 순회
   ├─ 짧은 문단 → 청크에 누적
   └─ 긴 문단 → 문장 단위로 분해 후 청크화
```

### `normalize_text`

- 앞뒤 공백 제거
- 여러 줄 공백 → 문단 단위(`\n\n`)로 정리  
    → **문단 경계 안정화**

### `split_paragraphs`
- `\n\n` 기준으로 문단 분리
- 빈 문단 제거

### 핵심 전략 ①: 문단 기반 누적 청킹

- 현재 청크 길이(`current_len`) + 문단 길이 ≤ `max_len`  
    → 같은 청크에 추가
- 초과하면:
    - 기존 청크 확정
    - overlap 설정 시 **이전 문단 일부를 다음 청크에 포함**
👉 **문단을 최대한 보존**하면서 길이 제한만 적용

### 핵심 전략 ②: 긴 문단 처리 (`chunk_long_paragraph`)

문단 자체가 max_len 초과 시
1. 문단을 문장 단위로 분리
2. 문장을 하나씩 누적
3. 길이 초과 시:
    - 현재 문장 묶음을 청크로 확정
    - overlap 수만큼 **이전 문장 재사용**
    - 다음 청크 시작

Overlap 전략의 정확한 의미

- `overlap = N`
    - 이전 청크의 **마지막 N개 문단 or 문장**을
    - 다음 청크의 앞부분에 재사용

- 목적:
    - 벡터 검색 시 **경계 정보 손실 방지**
    - RAG 응답에서 문맥 단절 완화
        

### embedding 코드 설명 및 /test-embed 호출

-  `embedding.py` — 임베딩 처리 핵심 흐름
- 환경변수 `EMBED_MODEL`로 **SentenceTransformer 모델 로드**
- **ingest용 / query용 임베딩을 분리**해서 생성

### 처리 방식
- 입력 텍스트 앞에 역할 프리픽스 추가
    - 문서: `"passage: {text}"`
    - 질의: `"query: {text}"`
- SentenceTransformer로 벡터 생성
- 결과를 **list 형태의 embedding 벡터**로 반환

### /search, /query API 호출

- `/search` API (vector search 전용)
	-  코드 흐름
		1. **질문(q)을 query embedding으로 변환**
		2. **Qdrant에 벡터 검색 요청**
		3. **top_k개 결과 반환**
		4. 각 결과에서
		    - 유사도 점수
		    - 텍스트
		    - 출처
		    - 청크 ID  
			만 추려서 응답

- `/query` API (완전한 RAG 파이프라인)
	- 코드 흐름
		1. 질문(q) 입력
		2. Retriever로 관련 context 검색
		3. 검색 결과로 프롬프트 생성
		4. LLM 호출 → 답변 생성
		5. 답변 + 근거(context) 함께 반환

###  도커 컴포즈 실행

`docker compose up --build -d`


## Streamlit UI 및 시연

프론트엔드에서는 Streamlit을 사용해  
질문을 입력하면

- 답변
    
- 근거로 사용된 chunk
    
- 유사도 점수  
    를 한 번에 확인할 수 있도록 UI를 구성했습니다.
    

---

## 트러블슈팅

마지막으로 트러블슈팅입니다.

첫째, **임베딩 모델 변경**입니다.  
`all-MiniLM-L6-v2`에서  
`multilingual-e5-small`로 교체하여  
**한국어 검색 품질을 개선**했습니다.

둘째, **Chunk 전략 변경**입니다.  
고정 길이 기반 청킹에서  
**문단 기반, 최대 길이 제한 방식**으로 변경해  
문맥 보존 성능을 개선했습니다.

---

## 마무리

정리하자면,  
이번 프로젝트를 통해

- RAG의 개념을 이론이 아닌 **구현 관점에서 이해**할 수 있었고
    
- Agent나 프레임워크에 의존하지 않고  
    **RAG 파이프라인을 직접 설계·튜닝해볼 수 있는 경험**을 할 수 있었습니다.
    

이상으로 발표를 마치겠습니다.  
감사합니다.