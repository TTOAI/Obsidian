# ğŸš€ **Step 6-1 â€” LLM í˜¸ì¶œ ì„œë¹„ìŠ¤ ë§Œë“¤ê¸°**

LLMì„ í˜¸ì¶œí•˜ëŠ” ì „ìš© íŒŒì¼ì„ ë§Œë“ ë‹¤.

ğŸ“ `backend/app/services/llm_client.py` ìƒì„±:

```python
import os
from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def ask_llm(prompt: str, model: str = "gpt-4o-mini"):
    """
    OpenAI GPT ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•œë‹¤.
    """
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."},
            {"role": "user", "content": prompt}
        ]
    )

    return response.choices[0].message["content"]
```

---

# ğŸš€ **Step 6-2 â€” ìµœì¢… RAG ì—”ë“œí¬ì¸íŠ¸ ë§Œë“¤ê¸°**

ì´ì œ Retrieval + Prompt + LLM í˜¸ì¶œì„ í•©ì¹œ ìµœì¢… APIë¥¼ ë§Œë“ ë‹¤.

ğŸ“ `backend/app/routers/query.py` (ì¶”ê°€):

```python
from fastapi import APIRouter
from app.services.retriever import Retriever
from app.services.prompter import build_prompt
from app.services.llm_client import ask_llm

router = APIRouter()

@router.get("/query")
def query(q: str):
    # 1) ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    retriever = Retriever(top_k=5)
    contexts = retriever.retrieve(q)

    # 2) prompt êµ¬ì„±
    prompt = build_prompt(q, contexts)

    # 3) LLMì—ê²Œ ë‹µë³€ ìš”ì²­
    answer = ask_llm(prompt)

    return {
        "query": q,
        "contexts_used": len(contexts),
        "answer": answer
    }
```

ì´ì œ `/query` í•˜ë‚˜ë¡œ ëª¨ë“  ë‹¨ê³„ê°€ ìë™ ì‹¤í–‰ëœë‹¤.

---

# ğŸš€ **Step 6-3 â€” í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**

`OPENAI_API_KEY`ê°€ Docker í™˜ê²½ì— ìˆì–´ì•¼ í•œë‹¤.

ğŸ“ `docker-compose.yml`:

```yaml
environment:
  - OPENAI_API_KEY=ì—¬ê¸°ì—_ì‹¤ì œ_í‚¤
```

---

# ğŸš€ **Step 6-4 â€” Docker ì¬ì‹œì‘ í›„ í…ŒìŠ¤íŠ¸**

```bash
docker compose down
docker compose up --build -d
```

---

# ğŸš€ **Step 6-5 â€” ì „ì²´ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸**

### â‘  ë¬¸ì„œ ingest

POST `/ingest`

### â‘¡ ì§ˆë¬¸í•˜ê¸°

GET:

```
/query?q=ë¬¸ì„œ ê´€ë¦¬ ì‹œìŠ¤í…œì´ë€?
```

ì˜ˆìƒ ì‘ë‹µ:

```json
{
  "query": "ë¬¸ì„œ ê´€ë¦¬ ì‹œìŠ¤í…œì´ë€?",
  "contexts_used": 3,
  "answer": "ë¬¸ì„œ ê´€ë¦¬ ì‹œìŠ¤í…œì€..."
}
```

LLMì´ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ì„ ìƒì„±í•˜ê¸° ì‹œì‘í•œë‹¤.

---

# ğŸ‰ **6ë‹¨ê³„ê°€ ëë‚˜ë©´ ìš°ë¦¬ëŠ” ì–´ë–¤ ìƒíƒœì¼ê¹Œ?**

| ê¸°ëŠ¥                     | ì™„ì„± ì—¬ë¶€ |
| ---------------------- | ----- |
| ê²€ìƒ‰ëœ ë¬¸ì„œ ê¸°ë°˜ prompt ìƒì„±    | âœ”     |
| prompt â†’ LLM ì „ë‹¬        | âœ”     |
| OpenAI GPTë¡œ ë‹µë³€ ìƒì„±      | âœ”     |
| ìµœì¢… `/query` RAG API ì™„ì„± | âœ”     |
| RAG íŒŒì´í”„ë¼ì¸ ì™„ì „ì²´          | âœ”     |

ì¦‰, **Retrieval + Generationì´ ê²°í•©ëœ ì™„ì„±í˜• RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!**

---
