
### Part1 BERT 모델 코드 정리

- `skt/kobert-base-v1` 모델을 사용
- `gluonnlp.data.BERTSPTokenizer`(SentencePiece 기반 KoBERTSPTokenizer)를 토크나이저로 사용
- `get_tokenizer()`
	- `kobert.utils.get_tokenizer()` 함수
	- `tokenizer_78b3253a26.model` (SentencePiece 모델 파일 경로)을 반환함

### 코드 흐름

```bash
# main.py 실행

python main.py --yaml_config ./configs/${modelname}/${modelname}-train.yaml
```

```python
# main.py

if __name__=='__main__':
    parser = argparse.ArgumentParser(description='Fake News Detection - Task1')
    parser.add_argument('--yaml_config', type=str, default=None, help='exp config file')    
  
    args = parser.parse_args()
  
    # config
    cfg = yaml.load(open(args.yaml_config,'r'), Loader=yaml.FullLoader)
  
    run(cfg)
```

```yaml
# BERT-train.yaml

TOKENIZER:
    name: bert
```

```python
# main

def run(cfg):
	tokenizer, word_embed = create_tokenizer(
		name            = cfg['TOKENIZER']['name'],
		vocab_path      = cfg['TOKENIZER'].get('vocab_path', None),
		max_vocab_size  = cfg['TOKENIZER'].get('max_vocab_size', None)
	)
	...
```

```yaml
# requirements.txt

kobert @ git+https://git@github.com/SKTBrain/KoBERT.git@47a69af87928fc24e20f571fe10c3cc9dd9af9a3

mxnet==1.7.0.post2
```

```python
# dataset/factory.py

from kobert import get_pytorch_kobert_model
from kobert.utils import get_tokenizer

def create_tokenizer(name: str, vocab_path: str, max_vocab_size: int):
    # ...
    elif name == 'bert':
        word_embed = None
        _, vocab = get_pytorch_kobert_model(cachedir=".cache")
        tokenizer = nlp.data.BERTSPTokenizer(get_tokenizer(), vocab, lower=False)
        
    return tokenizer, word_embed
```

```yaml
# BERT-train.yaml

MODEL:
    modelname: bert
    PARAMETERS:
        pretrained_name: 'skt/kobert-base-v1'
        num_classes: 2
    CHECKPOINT:
        checkpoint_path: null
```

```python
# main.py

def run(cfg):
	...
	model = create_model(
		modelname                 = cfg['MODEL']['modelname'],
		hparams                   = cfg['MODEL']['PARAMETERS'],
		word_embed                = word_embed,
		tokenizer                 = tokenizer,
		freeze_word_embed         = cfg['MODEL'].get('freeze_word_embed',False),
		use_pretrained_word_embed = cfg['MODEL'].get('use_pretrained_word_embed',False),
		checkpoint_path           = cfg['MODEL']['CHECKPOINT']['checkpoint_path'],
	)
	model.to(device)
	...
```

```python
# models/factory.py

import torch
from .registry import is_model, model_entrypoint

def create_model(
        modelname: str,
        hparams: dict,
        word_embed = None,
        tokenizer = None,
        freeze_word_embed: bool = False,
        use_pretrained_word_embed: bool = False,
        checkpoint_path: str = None,
        **kwargs
    ):
    
    if not is_model(modelname):
        raise RuntimeError('Unknown model (%s)' % modelname)
  
    create_fn = model_entrypoint(modelname)
    
    model = create_fn(
        hparams    = hparams
    )
  
    # word embedding
    if use_pretrained_word_embed:
        _logger.info('load pretrained word embedding')
        model.init_w2e(word_embed, len(tokenizer.special_tokens))
    
    # freeze word embedding
    if freeze_word_embed:
        _logger.info('freeze pretrained word embedding')
        model.freeze_w2e()
  
    # load checkpoint weights
    if checkpoint_path:
        _logger.info('load a trained model weights from {}'.format(checkpoint_path))
        model.load_state_dict(torch.load(checkpoint_path))
 
    return model
```

```python
# registry.py

def is_model(model_name):
    """ Check if a model name exists
    """
    return model_name in _model_entrypoints
```

```python
# models/bert.py

from transformers import AutoConfig

@register_model
def bert(hparams: dict, **kwargs):
    model_config = AutoConfig.from_pretrained(hparams['pretrained_name'])
    model = BERT(
        pretrained_name = hparams['pretrained_name'],
        config          = model_config,
        num_classes     = hparams['num_classes']
    )
 
    return model
```

- `@register_model`: `파이썬 데코레이터(@decorator)` 문법
- 실행 순서
	- 파이썬이 `def bert(...):` 함수를 정의하려고 함
	- 그런데 바로 위에 `@register_model`이 붙어 있음
	- 그래서 파이썬은 `bert` 함수 완성 후 그 함수를 `register_model(bert)` 형태로 호출함
	- 이때 `register_model()`의 매개변수 `fn`에는 방금 정의된 `bert` 함수 객체가 들어감
	- `register_model()`은 내부에서 여러 등록 작업을 수행한 뒤 마지막에 `return fn`을 반환함
	- 그 결과, 원래의 `bert` 함수는 `register_model()`이 반환한 함수로 대체됨

```python
# models.py

def register_model(fn):
    # lookup containing module
    mod = sys.modules[fn.__module__]
    module_name_split = fn.__module__.split('.')
    module_name = module_name_split[-1] if len(module_name_split) else ''
  
    # add model to __all__ in module
    model_name = fn.__name__
    if hasattr(mod, '__all__'):
        mod.__all__.append(model_name)
    else:
        mod.__all__ = [model_name]
  
    # add entries to registry dict/sets
    _model_entrypoints[model_name] = fn
    _model_to_module[model_name] = module_name
    _module_to_models[module_name].add(model_name)
  
    return fn
```

- __module__: 파이썬 객체의 `메타데이터 속성` 중 하나

```python
# models/bert.py
from transformers import BertModel, BertPreTrainedModel
import torch.nn as nn

class BERT(BertPreTrainedModel):
    def __init__(self, pretrained_name: str, config: dict, num_classes: int):
        super().__init__(config)
  
        self.bert = BertModel.from_pretrained(pretrained_name, config=config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, num_classes)
```

```python
# registry.py

def model_entrypoint(model_name):
    """Fetch a model entrypoint for specified model name
    """
    return _model_entrypoints[model_name]
```

```python
# main.py

def run(cfg):
	...
	if cfg['MODE']['do_train']:
		...
	elif cfg['MODE']['do_test']:
		...
```

