# 📌 1. 프로젝트 한 줄 정의

**“문서(PDF, TXT, JSON 등)를 임베딩하여 Qdrant VectorDB에 저장하고, 자연어 질문을 FastAPI 서버로 받아 관련 문서를 검색한 뒤 LLM이 근거 기반 답변을 생성하는 RAG Q&A 시스템 구축 프로젝트.”**

---

# 📌 2. 프로젝트 목표

1. **VectorDB 기반 검색 시스템을 처음부터 끝까지 직접 구성한다.**
    - Chunking → Embedding → Vector 저장 → Semantic Search
2. **RAG(Retrieval-Augmented Generation)의 전체 파이프라인을 구현한다.**
    - Query Embedding → Vector 검색 → 문서 컨텍스트 기반 LLM 응답
3. **실제 서비스 형태(FastAPI API + Docker + Qdrant)로 동작하는 MVP 구축**
4. **20분 영상 강의로 설명 가능한 구조(명확한 아키텍처 & 시연 중심)**

---

# 📌 3. 전체 개념 흐름(High-level Architecture)

### 🔸 1) 문서 Ingestion 파이프라인

```
문서(PDF/MD/TXT) → 텍스트 추출 → Chunking → Embedding → Qdrant에 저장
```

- **Chunking**: 300~800자 단위
- **Metadata**: 제목, 구분, 페이지 번호, 태그 등
- **Embedding**: OpenAI text-embedding-3-large 또는 Sentence-Transformer
- **VectorDB 저장**: upsert(points) 형태로 Qdrant 컬렉션에 저장

---

### 🔸 2) Query → Retrieval → LLM 답변 과정

```
사용자 질문 → Query Embedding → Vector 검색 → Top-K Chunk
           → Prompt 구성 → LLM 답변 생성 → 응답 반환
```

- 검색된 문서 chunk들을 **LLM에게 컨텍스트로 전달**
- LLM은 “컨텍스트 기반으로만 답하라”고 명시
- 최종 응답에는 “사용된 문서 snippet + 출처" 포함

---

### 🔸 3) 인프라 구성(Docker 기반)

```
docker-compose
│
├── FastAPI backend (RAG API Server)
├── Qdrant VectorDB
└── (선택) Frontend UI (React 또는 최소 HTML)
```

---

# 📌 4. 코드 구성(프로젝트 구조)

```
rag-qdrant-project/
│
├── backend/
│   ├── app/
│   │   ├── main.py                   # FastAPI 엔트리포인트
│   │   ├── routers/
│   │   │   ├── ingest.py             # 문서 업로드/벡터화 API
│   │   │   └── query.py              # RAG 질의 API
│   │   ├── services/
│   │   │   ├── chunker.py            # 텍스트 chunking
│   │   │   ├── embedding.py          # 임베딩 생성
│   │   │   ├── qdrant_client.py      # Qdrant 검색/저장
│   │   │   ├── rag_pipeline.py       # end-to-end RAG 파이프라인
│   │   │   └── llm_client.py         # LLM API 호출
│   │   ├── models/
│   │   │   ├── domain.py
│   │   │   └── schemas.py            # API Request/Response 정의
│   │   └── utils/
│   │       └── text_clean.py
│   └── Dockerfile
│
├── docker-compose.yml
└── README.md
```

---

# 📌 5. 주요 기술 스택

|기술|역할|
|---|---|
|**Qdrant**|문서 chunk 벡터 저장 및 Semantic Search|
|**FastAPI**|/ingest, /query API 서버|
|**OpenAI Embedding**|텍스트 → 벡터 변환|
|**Any LLM (GPT-4o mini 등)**|문서 기반 답변 생성|
|**Docker Compose**|로컬 환경에서 전체 인프라 구성|

---

# 📌 6. 최종 결과물(MVP)

### ✔ 기능 1: 문서 업로드 후 자동 벡터화

```
POST /ingest
{
  "source": "sample.pdf",
  "text": "문서 내용…"
}
```

### ✔ 기능 2: 자연어 질문 → 정확한 문서 기반 답변

```
POST /query
{
  "question": "이 문서에서 API 설계 원칙을 정리해줘."
}
```

LLM 응답 예시(첨부파일 17page 기반):  

```
{
  "answer": "문서에서는 API를 ... 원칙에 따라 구성해야 한다고 설명합니다...",
  "sources": [
    {"snippet": "API 설계 원칙은 ...", "score": 0.89}
  ]
}
```

---

# 📌 7. 20분 영상 강의 구성안(초안)

### **0:00–1:30** RAG가 필요한 이유

- LLM은 지식을 외우지 않는다 → 검색이 필요
- 문서 기반 Q&A 서비스의 장점

### **1:30–4:00** 시스템 전체 구조 설명

- Ingestion + Retrieval + Generation 전체 흐름 그림

### **4:00–8:00** 인프라 & 코드 구조 소개

- docker-compose
- FastAPI + Qdrant + Embedding 구조 설명

### **8:00–14:00** 실전 코드 설명

- chunker.py
- embedding.py
- rag_pipeline.py
- Qdrant 검색 과정

### **14:00–18:00** 데모

- 문서 ingest
- /query 호출 → LLM 답변 & 출처 보기

### **18:00–20:00** 마무리 & 확장 아이디어

- 멀티 소스 연결
- PDF, 웹 크롤링 추가
- LangChain/LlamaIndex 적용
- 실제 기업 적용 사례

---

# 📌 8. 프로젝트 완성 후 기대 효과

- 실제 기업에서 가장 수요 높은 AI 시스템: **문서 기반 RAG**
- 검색 + LLM 결합 엔지니어링 경험 확보
- 벡터DB와 LLM API 기반 실전 프로젝트
- AI 백엔드 / RAG 엔지니어 포지션과 높은 연관도
- 포트폴리오 가치 매우 큼

---
