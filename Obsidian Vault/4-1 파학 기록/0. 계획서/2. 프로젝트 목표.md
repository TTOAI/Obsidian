
## 입력 프롬프트 예시

- 정형화된 API 명세와 고정된 출력 목표만을 입력으로 제공
- API 명세 예시
```json
{
  "endpoints": [
    {
      "name": "createUser",
      "method": "POST",
      "path": "/users",
      "request": { "name": "string", "age": "number" },
      "response": { "id": "number" },
      "errors": ["400", "409"]
    }
  ]
}
```
- 출력 목표 정의 예시
```
- 언어: Python
- 프레임워크: FastAPI
- 테스트 기준: 모든 API 단위 테스트 통과
- 출력 형식: 실행 가능한 프로젝트 디렉터리
```

- 테스트 시나리오 생성과 코드 수정 과정을 AI 에이전트가 자율적으로 수행

- "AI는 어느 수준의 테스트 지원까지 자율 개발이 가능한가?" 확인 가능

---

## 수정 에이전트?

- (1) 자기합리화(Self-justification) 문제
	LLM은 이런 경향이 있다:
	- 내가 만든 코드 → 실패
	- 내가 직접 고치라고 하면 →  
		실패 원인을 축소하거나, 기존 결정을 정당화하려는 방향으로 수정
	그래서 연구 쪽에서는:
	- “처음 만든 에이전트”
	- “남이 만든 걸 고치는 에이전트”
	를 나눠서 인지적 편향을 줄이려는 시도를 한다.
	이게 “수정 에이전트 분리”의 가장 큰 이론적 배경이야.

하지만... 기존의 4-에이전트 구조 고집 이유

- (1) 구조가 직관적이다
	심사자 입장에서:
	- 입력
	- 구현
	- 검증
	- 출력
	-> 인간 개발 프로세스와 완전히 대응
	“수정 에이전트”는 설명 없으면 오히려 헷갈릴 수 있다.

- (3) 연구 초점이 흐려지지 않는다
	너희 프로젝트의 핵심 질문은:
	“AI가 개발 루프를 자율적으로 돌 수 있는가?”
	이지,
	“에이전트를 몇 개로 나누는 게 최적일까?”
	가 아니야.
	-> 수정 에이전트 분리는 **부차적 실험 변수**일 뿐.

---

## MCP와 A2A의 적용

- 핵심 개념: 컨텍스트 분리, 도구 호출의 명시화, 에이전트 간 역할 및 책임의 구조화

- 컨텍스트 분리
	- LLM이 알고 있는 정보와 시스템이 가지고 있는 정보를 구분하지 않으면 신뢰 가능한 검증이 불가능하다.
	- 구현 에이전트와 검증 에이전트 간 정보를 직접 보지 못한다.
	- 누가 무엇을 알고 판단했는지를 시스템이 통제한다.
	- 이게 컨텍스트 분리.

- 도구 호출의 명시화
	- 기존 방식 (LLM)
		- "코드를 실행해보고 결과를 확인해줘"라고 한다면,
		- LLM이 실행했다고 가정하고 결과를 추론해서 말함.
		- 환각 발생 지점임.
	- MCP
		- LLM은 실행하지 말고, 실행을 ‘요청’만 하게 하자.
		- 에이전트가 함수와 같은 도구 호출만 선언하면,
		- 실제 실행은 Python 프로세스나 테스트 러너가 담당함.
		- 결과를 JSON 로그로 반환하며 에이전트는 실행 결과를 상상하지 못함.
		- 이게 도구 호출의 명시화.

- 에이전트 간 역할 및 책임의 구조화
	- 각 에이전트가 무엇을 책임지는지가 명확해야 전체 시스템의 행동을 예측할 수 있다.
	- 우리 프로젝트
		- 입력 에이전트: 입력 정규화
		- 구현 에이전트: 코드 생성
		- 검증 에이전트: 통과/실패 판정
		- 출력 에이전트: 결과 정리
		- 에이전트가 역할에 따라 제한된 판단만 수행함.
		- 이게 역할 및 책임의 구조화.

---

